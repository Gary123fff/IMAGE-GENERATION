# -*- coding: utf-8 -*-
"""IMAGE GENERATION

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GvTEAw2ZjTnom2wa1UsThKe3Psie8kM6
"""

import math
import os
import torch
import torchvision
from PIL import Image
from matplotlib import pyplot as plt
from torch.utils.data import DataLoader
import random
import imageio
import numpy as np
from argparse import ArgumentParser

from tqdm.auto import tqdm

import torch.nn as nn
from torch.optim import Adam

from torchvision.transforms import Compose, ToTensor, Lambda,Resize,Normalize
from torchvision.datasets.mnist import MNIST, FashionMNIST
from torch.utils.data import DataLoader
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import datasets
from torchvision import models
from torchsummary import summary
from torchvision import transforms
from torchvision.models.inception import inception_v3
from scipy.stats import entropy
from torch.cuda.amp import GradScaler
import copy

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from google.colab import drive
drive.mount('/content/drive')

from torchvision.io import read_image
def read_image_into_tuple(main_dir):
  o=0
  sub_dir = os.listdir(main_dir)
  out = []
  label =[]
  tf = transforms.Compose([
    transforms.Resize([256,256])
])

  for dir in sub_dir:
    for img in tqdm(os.listdir(f"{main_dir}/{dir}")):
      a = read_image(f"{main_dir}/{dir}/{img}")
      a= 2 * a / 255. - 1
      out.append((a.float(),o))
    print(o)
    o+=1
    if(o==10):
      break

  return out

"""
a = read_image_into_tuple("/content/drive/MyDrive/imagenet1k")

loader = DataLoader(a, 16, shuffle=True,
                           num_workers=4, persistent_workers=True, pin_memory=True)"""

pip install diffusers

# Commented out IPython magic to ensure Python compatibility.
from fastdownload import FastDownload
## Imaging  library
from PIL import Image
from torchvision import transforms as tfms
## Basic libraries
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
## Loading a VAE model
from diffusers import AutoencoderKL
vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")

def pil_to_latents(image):
    '''
    Function to convert image to latents
    '''
    init_image = image * 2.0 - 1.0

    init_image = init_image.to(device="cuda", dtype=torch.float16)
    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215
    return init_latent_dist
def latents_to_pil(latents):
    '''
    Function to convert latents to images
    '''
    latents = (1 / 0.18215) * latents
    with torch.no_grad():
        image = vae.decode(latents.half()).sample


    image=image.to(torch.float32)

    image = (image / 2 + 0.5).clamp(0, 1)

    image = image.detach().cpu().numpy()

    return image

"""for e,(i,label) in enumerate(tqdm(loader)):

  i=pil_to_latents(i.permute(0,1,2,3)).to(device)

  torch.save([i,label], os.path.join(
            "/content/drive/MyDrive/imagenet100data/", 'ckpt_' + str(e) + "_.pt"))"""



"""STORE_PATH_MNIST = f"ddpm_model_mnist.pt"
STORE_PATH_FASHION = f"ddpm_model_fashion.pt"
no_train = False
fashion = False
imagesize=120

batch_size = 64
lr = 0.001
n_epochs=500
step=1000
gnumphoto=64
store_path = "ddpm_fashion.pt" if fashion else "ddpm_mnist.pt"

tf = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize([120,120]),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_set = torchvision.datasets.Food101('data', download=True, transform=tf)
loader = DataLoader(train_set, batch_size, shuffle=True,
                           num_workers=4, persistent_workers=True, pin_memory=True)

"""
#STORE_PATH_MNIST = f"ddpm_model_mnist.pt"
STORE_PATH_FASHION = f"ddpm_model_fashion.pt"
no_train = False
fashion = False
imagesize=96

batch_size = 32
lr = 0.001
n_epochs=500
step=500
gnumphoto=64
store_path = "ddpm_fashion.pt" if fashion else "ddpm_mnist.pt"

tf = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize([256,256]),

    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_set = datasets.STL10('data', download=True, transform=tf)
loader = DataLoader(train_set, batch_size, shuffle=True,
                           num_workers=4, persistent_workers=True, pin_memory=True)
"""

SEED = 0
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

STORE_PATH_MNIST = f"ddpm_model_mnist.pt"
STORE_PATH_FASHION = f"ddpm_model_fashion.pt"
no_train = False
fashion = False
lr = 0.0001

store_path = "ddpm_fashion.pt" if fashion else "ddpm_mnist.pt"
batch_size = 80
gnumphoto=64
imagesize=32
step=500
tf = transforms.Compose([
    transforms.ToTensor(),
    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

train_set = datasets.CIFAR10('data', train=True, download=True, transform=tf)
loader = DataLoader(train_set, batch_size, shuffle=True,
                           num_workers=4, persistent_workers=True, pin_memory=True)

"""
model=torch.load("/content/drive/MyDrive/model6cifarselfh")
diffusion = Diffusion(device=device)
x_train = diffusion.sample(model, 1)
reverse_transforms = transforms.Compose([
  transforms.ToPILImage(),
])

x_train = (x_train * 255).type(torch.uint8)
print(x_train)

image = x_train[0, :, :, :]
plt.imshow(reverse_transforms(image))
"""

class MyBlock(nn.Module):
    def __init__(self,in_channels,out_channels,channel=None,padding=1, activation=None, normalize=True,residual=False,dropout=0,attn=True):
        super(MyBlock, self).__init__()
        self.relu=nn.ReLU()
        if not channel:
            channel = out_channels
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, channel, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(1, channel),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Conv2d(channel, out_channels, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(1, out_channels)
        )
        self.emb_layer = nn.Sequential(
            nn.SiLU(),
            nn.Linear(in_features=256, out_features=out_channels),
        )
        self.cond_proj = nn.Sequential(
            nn.SiLU(),
            nn.Linear(in_features=256, out_features=out_channels),
        )
        self.residual=residual
        if attn==True:
          self.attn=SelfAttention(out_channels)
        else:
            self.attn = nn.Identity()
    def forward(self, x,t,l):
      t=self.emb_layer(t)

      t = t.view(t.shape[0], t.shape[1], 1, 1).repeat(1, 1, x.shape[-2], x.shape[-1])
      l=self.cond_proj(l)

      l = l.view(l.shape[0], l.shape[1], 1, 1).repeat(1, 1, x.shape[-2], x.shape[-1])
      if self.residual:
          return self.attn(self.relu(x+self.double_conv(x)+t+l))
      else:
        return self.attn(self.double_conv(x)+t+l)
class ConditionalEmbedding(nn.Module):
    def __init__(self, num_labels, d_model, dim):
        assert d_model % 2 == 0
        super().__init__()
        self.condEmbedding = nn.Sequential(
            nn.Embedding(num_embeddings=num_labels + 1, embedding_dim=d_model, padding_idx=0),
            nn.Linear(d_model, dim),
            nn.SiLU(),
            nn.Linear(dim, dim),
        )

    def forward(self, t):
        emb = self.condEmbedding(t)
        return emb
class TransformerEncoderSA(nn.Module):
    def __init__(self, num_channels: int, size: int, num_heads: int = 4):



        super(TransformerEncoderSA, self).__init__()
        self.num_channels = num_channels
        self.size = size
        self.mha = nn.MultiheadAttention(embed_dim=num_channels, num_heads=num_heads, batch_first=True)
        self.ln = nn.LayerNorm([num_channels])
        self.ff_self = nn.Sequential(
            nn.LayerNorm([num_channels]),
            nn.Linear(in_features=num_channels, out_features=num_channels),
            nn.LayerNorm([num_channels]),
            nn.Linear(in_features=num_channels, out_features=num_channels)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:





        x = x.view(-1, self.num_channels, self.size * self.size).permute(0, 2, 1)
        x_ln = self.ln(x)
        attention_value, _ = self.mha(query=x_ln, key=x_ln, value=x_ln)
        attention_value = attention_value + x
        attention_value = self.ff_self(attention_value) + attention_value
        return attention_value.permute(0, 2, 1).view(-1, self.num_channels, self.size, self.size)
class SelfAttention(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.group_norm = nn.GroupNorm(32, in_ch)
        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)

    def forward(self, x):
        B, C, H, W = x.shape
        h = self.group_norm(x)
        q = self.proj_q(h)
        k = self.proj_k(h)
        v = self.proj_v(h)

        q = q.permute(0, 2, 3, 1).view(B, H * W, C)
        k = k.view(B, C, H * W)
        w = torch.bmm(q, k) * (int(C) ** (-0.5))
        assert list(w.shape) == [B, H * W, H * W]
        w = F.softmax(w, dim=-1)

        v = v.permute(0, 2, 3, 1).view(B, H * W, C)
        h = torch.bmm(w, v)
        assert list(h.shape) == [B, H * W, C]
        h = h.view(B, H, W, C).permute(0, 3, 1, 2)
        h = self.proj(h)

        return x + h
class Res(nn.Module):
    def __init__(self, in_channels,out_channels,dropout,attn=True):
        super(Res, self).__init__()
        self.maxpool_conv =nn.MaxPool2d(kernel_size=(2, 2))

        self.conv1=MyBlock(in_channels, in_channels,dropout=dropout,residual=True,attn=attn)
        self.conv2=MyBlock(in_channels, out_channels,dropout=dropout,attn=attn)
        self.emb_layer = nn.Sequential(
            nn.SiLU(),
            nn.Linear(in_features=256, out_features=out_channels),
        )
        self.cond_proj = nn.Sequential(
            nn.SiLU(),
            nn.Linear(in_features=256, out_features=out_channels),
        )
    def forward(self, x,t,l):


        x=self.conv1(x,t,l)
        x=self.conv2(x,t,l)
        return x

class Res2(nn.Module):
    def __init__(self, in_channels,out_channels,dropout=0,attn=True):
        super(Res2, self).__init__()
        self.b1=MyBlock(in_channels, in_channels,dropout=dropout,attn=attn)
        self.b2=MyBlock(in_channels, out_channels,dropout=dropout,attn=attn)
        self.b3=MyBlock(out_channels, in_channels,dropout=dropout,attn=attn)
    def forward(self, x, t,l):
        input=x
        x=self.b1(x,t,l)
        x=self.b2(x,t,l)
        x=self.b3(x,t,l)
        return x

class Res3(nn.Module):
    def __init__(self, in_channels,out_channels,mid,dropout=0,attn=True):
        super(Res3, self).__init__()

        self.conv1 =MyBlock(in_channels=in_channels, out_channels=in_channels,residual=True,dropout=dropout,attn=attn)
        self.conv2 =MyBlock(in_channels=in_channels, out_channels=out_channels, channel=mid,dropout=dropout,attn=attn)


        self.emb_layer = nn.Sequential(
            nn.SiLU(),
            nn.Linear(in_features=256, out_features=out_channels),
        )
        self.cond_proj = nn.Sequential(
            nn.SiLU(),
            nn.Linear(in_features=256, out_features=out_channels),
        )
    def forward(self, x,t,l):
        x=self.conv1(x,t,l)
        x=self.conv2(x,t,l)
        return x
class UpSample(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)



    def forward(self, x, temb):
        _, _, H, W = x.shape
        x = F.interpolate(
            x, scale_factor=2, mode='nearest')
        x = self.main(x)
        return x
class DownSample(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)



    def forward(self, x, temb):
        x = self.main(x)
        return x
def sinusoidal_embedding(n, d):
    # Returns the standard positional embedding
    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])
    sin_mask = torch.arange(0, n, 2)

    embedding[sin_mask] = torch.sin(embedding[sin_mask])
    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])

    return embedding

class Net(nn.Module):
    def __init__(self, n_steps=500, time_emb_dim=256,num_b=2,ch_m=[1],dropout=0,ch=32,num_l=0):
        super(Net, self).__init__()
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.maxpool_conv=nn.MaxPool2d(kernel_size=(2,2))
        # Sinusoidal embedding
        self.time_embed = nn.Embedding(n_steps, time_emb_dim)
        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)
        self.time_embed.requires_grad_(False)

        self.cond_embedding = ConditionalEmbedding(num_l, n_steps, time_emb_dim)

        chs = [ch]
        now_ch = ch

        self.down = nn.ModuleList()
        self.head =nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)

        for i,mu in enumerate(ch_m):
          out_ch = ch * mu
          for _ in range(num_b):
            self.down.append(Res(
                    in_channels=now_ch, out_channels=out_ch,
                    dropout=dropout, attn=False))
            now_ch = out_ch
            chs.append(now_ch)
          if i != len(ch_m) - 1:
                self.down.append(DownSample(now_ch))
                chs.append(now_ch)



        self.middle= nn.ModuleList([
            Res2(in_channels=now_ch, out_channels=now_ch, dropout=dropout, attn=False)
        ])


        self.up = nn.ModuleList()
        for i, mult in reversed(list(enumerate(ch_m))):
            out_ch = ch * mult
            for _ in range(num_b + 1):

                self.up.append(Res3(
                    in_channels=chs.pop() + now_ch,mid=out_ch*2, out_channels=out_ch,dropout=dropout, attn=False))
                now_ch = out_ch
            if i != 0:
                self.up.append(UpSample(now_ch))
        assert len(chs) == 0


        self.out_conv = nn.Conv2d(in_channels=out_ch, out_channels=3, kernel_size=(1, 1))


    def forward(self, x, t,l):

        temb = self.time_embed(t)
        l=self.cond_embedding(l)



        h = self.head(x)
        hs = [h]
        for layer in self.down:
            h = layer(h, temb,l)
            hs.append(h)

        # Middle
        for layer in self.middle:

            h = layer(h, temb,l)

        # Upsampling
        for layer in self.up:

            if isinstance(layer, Res)or isinstance(layer, Res2) or isinstance(layer, Res3):
                h = torch.cat([h, hs.pop()], dim=1)

            h = layer(h, temb,l)
        h = self.out_conv(h)

        assert len(hs) == 0
        return h


    def _make_te(self, dim_in, dim_out):
        return nn.Sequential(
            nn.Linear(dim_in, dim_out),
            nn.SiLU(),
            nn.Linear(dim_out, dim_out)
        )

import math
from telnetlib import PRAGMA_HEARTBEAT
import torch
from torch import nn
from torch.nn import init
from torch.nn import functional as F


def drop_connect(x, drop_ratio):
    keep_ratio = 1.0 - drop_ratio
    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)
    mask.bernoulli_(p=keep_ratio)
    x.div_(keep_ratio)
    x.mul_(mask)
    return x

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)


class TimeEmbedding(nn.Module):
    def __init__(self, T, d_model, dim):
        assert d_model % 2 == 0
        super().__init__()
        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)
        emb = torch.exp(-emb)
        pos = torch.arange(T).float()
        emb = pos[:, None] * emb[None, :]
        assert list(emb.shape) == [T, d_model // 2]
        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)
        assert list(emb.shape) == [T, d_model // 2, 2]
        emb = emb.view(T, d_model)

        self.timembedding = nn.Sequential(
            nn.Embedding.from_pretrained(emb, freeze=False),
            nn.Linear(d_model, dim),
            Swish(),
            nn.Linear(dim, dim),
        )

    def forward(self, t):
        emb = self.timembedding(t)
        return emb


class ConditionalEmbedding(nn.Module):
    def __init__(self, num_labels, d_model, dim):
        assert d_model % 2 == 0
        super().__init__()
        self.condEmbedding = nn.Sequential(
            nn.Embedding(num_embeddings=num_labels + 1, embedding_dim=d_model, padding_idx=0),
            nn.Linear(d_model, dim),
            Swish(),
            nn.Linear(dim, dim),
        )

    def forward(self, t):
        emb = self.condEmbedding(t)
        return emb


class DownSample(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.c1 = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)
        self.c2 = nn.Conv2d(in_ch, in_ch, 5, stride=2, padding=2)

    def forward(self, x, temb, cemb):
        x = self.c1(x) + self.c2(x)
        return x


class UpSample(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.c = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)
        self.t = nn.ConvTranspose2d(in_ch, in_ch, 5, 2, 2, 1)

    def forward(self, x, temb, cemb):
        _, _, H, W = x.shape
        x = self.t(x)
        x = self.c(x)
        return x


class AttnBlock(nn.Module):
    def __init__(self, in_ch):
        super().__init__()
        self.group_norm = nn.GroupNorm(32, in_ch)
        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)
        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)

    def forward(self, x):
        B, C, H, W = x.shape
        h = self.group_norm(x)
        q = self.proj_q(h)
        k = self.proj_k(h)
        v = self.proj_v(h)

        q = q.permute(0, 2, 3, 1).view(B, H * W, C)
        k = k.view(B, C, H * W)
        w = torch.bmm(q, k) * (int(C) ** (-0.5))
        assert list(w.shape) == [B, H * W, H * W]
        w = F.softmax(w, dim=-1)

        v = v.permute(0, 2, 3, 1).view(B, H * W, C)
        h = torch.bmm(w, v)
        assert list(h.shape) == [B, H * W, C]
        h = h.view(B, H, W, C).permute(0, 3, 1, 2)
        h = self.proj(h)

        return x + h



class ResBlock(nn.Module):
    def __init__(self, in_ch, out_ch, tdim, dropout, attn=True):
        super().__init__()
        self.block1 = nn.Sequential(
            nn.GroupNorm(32, in_ch),
            Swish(),
            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),
        )
        self.temb_proj = nn.Sequential(
            Swish(),
            nn.Linear(tdim, out_ch),
        )
        self.cond_proj = nn.Sequential(
            Swish(),
            nn.Linear(tdim, out_ch),
        )
        self.block2 = nn.Sequential(
            nn.GroupNorm(32, out_ch),
            Swish(),
            nn.Dropout(dropout),
            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),
        )
        if in_ch != out_ch:
            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)
        else:
            self.shortcut = nn.Identity()
        if attn:
            self.attn = AttnBlock(out_ch)
        else:
            self.attn = nn.Identity()


    def forward(self, x, temb, labels):
        h = self.block1(x)
        h += self.temb_proj(temb)[:, :, None, None]
        h += self.cond_proj(labels)[:, :, None, None]
        h = self.block2(h)

        h = h + self.shortcut(x)
        h = self.attn(h)
        return h


class UNet(nn.Module):
    def __init__(self, T, num_labels, ch, ch_mult, num_res_blocks, dropout):
        super().__init__()
        tdim = ch * 4
        self.time_embedding = TimeEmbedding(T, ch, tdim)
        self.cond_embedding = ConditionalEmbedding(num_labels, ch, tdim)
        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)
        self.downblocks = nn.ModuleList()
        chs = [ch]  # record output channel when dowmsample for upsample
        now_ch = ch
        for i, mult in enumerate(ch_mult):
            out_ch = ch * mult
            for _ in range(num_res_blocks):
                self.downblocks.append(ResBlock(in_ch=now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout))
                now_ch = out_ch
                chs.append(now_ch)
            if i != len(ch_mult) - 1:
                self.downblocks.append(DownSample(now_ch))
                chs.append(now_ch)

        self.middleblocks = nn.ModuleList([
            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),
            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),
        ])

        self.upblocks = nn.ModuleList()
        for i, mult in reversed(list(enumerate(ch_mult))):
            out_ch = ch * mult
            for _ in range(num_res_blocks + 1):
                self.upblocks.append(ResBlock(in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout, attn=False))
                now_ch = out_ch
            if i != 0:
                self.upblocks.append(UpSample(now_ch))
        assert len(chs) == 0

        self.tail = nn.Sequential(
            nn.GroupNorm(32, now_ch),
            Swish(),
            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)
        )


    def forward(self, x, t, labels):
        # Timestep embedding
        temb = self.time_embedding(t)
        cemb = self.cond_embedding(labels)
        # Downsampling
        h = self.head(x)
        hs = [h]
        for layer in self.downblocks:
            h = layer(h, temb, cemb)
            hs.append(h)
        # Middle
        for layer in self.middleblocks:
            h = layer(h, temb, cemb)
        # Upsampling
        for layer in self.upblocks:
            if isinstance(layer, ResBlock):
                h = torch.cat([h, hs.pop()], dim=1)
            h = layer(h, temb, cemb)
        h = self.tail(h)

        assert len(hs) == 0
        return h

def extract(v, t, x_shape):
    device = t.device
    out = torch.gather(v, index=t, dim=0).float().to(device)
    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))
class GaussianDiffusionSampler(nn.Module):
    def __init__(self, model, beta_1, beta_T, T, w = 0.):
        super().__init__()

        self.model = model
        self.T = T
        ### In the classifier free guidence paper, w is the key to control the gudience.
        ### w = 0 and with label = 0 means no guidence.
        ### w > 0 and label > 0 means guidence. Guidence would be stronger if w is bigger.
        self.w = w

        self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())
        alphas = 1. - self.betas
        alphas_bar = torch.cumprod(alphas, dim=0)
        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]
        self.register_buffer('coeff1', torch.sqrt(1. / alphas))
        self.register_buffer('coeff2', self.coeff1 * (1. - alphas) / torch.sqrt(1. - alphas_bar))
        self.register_buffer('posterior_var', self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))

    def predict_xt_prev_mean_from_eps(self, x_t, t, eps):
        assert x_t.shape == eps.shape
        return extract(self.coeff1, t, x_t.shape) * x_t - extract(self.coeff2, t, x_t.shape) * eps

    def p_mean_variance(self, x_t, t, labels):
        var = torch.cat([self.posterior_var[1:2], self.betas[1:]])
        var = extract(var, t, x_t.shape)
        #labels=torch.repeat_interleave(labels,64*64).reshape(x_t.shape[0],1,64,64)/12.
        #x=torch.cat([x_t,labels],dim=1)
        eps = self.model(x_t, t,labels)
        labels=torch.zeros_like(labels).to(labels.device)
        #x=torch.cat([x_t,labels],dim=1)
        nonEps = self.model(x_t, t,labels)
        eps = (1. + self.w) * eps - self.w * nonEps
        xt_prev_mean = self.predict_xt_prev_mean_from_eps(x_t, t, eps=eps)
        return xt_prev_mean, var
    def forward(self, x_T, labels):
        x_t = x_T
        for time_step in reversed(range(self.T)):
            print(time_step)
            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.int64) * time_step

            mean, var= self.p_mean_variance(x_t=x_t, t=t, labels=labels)

            if time_step > 0:
                noise = torch.randn_like(x_t)
            else:
                noise = 0
            x_t = mean + torch.sqrt(var) * noise
            assert torch.isnan(x_t).int().sum() == 0, "nan in tensor."
        x_0 = x_t
        return torch.clip(x_0, -1, 1)

class IS:
  def __init__(self, number_photo=gnumphoto, splits=10,batch=batch_size,device="cuda"):
    self.number_photo = number_photo
    self.splits = splits
    self.batch=batch
  def get_pred(self,x):
    inception_model=inception_v3(pretrained=True,transform_input=False).cuda()
    inception_model.eval()
    up=nn.Upsample(size=(299,299),mode='bilinear',align_corners=False).cuda()
    x=up(x)
    x=inception_model(x)
    return F.softmax(x,dim=1).data.cpu().numpy()

  def score(self,model):
    preds=np.zeros((self.number_photo,1000))
    sampleph = diffusion.sample(model, self.number_photo)

    data=torch.tensor(sampleph/255).to(device)
    preds[0:64]=self.get_pred(data)
    split_scores=[]
    splits=10
    N=self.number_photo
    for k in range(splits):
      part=preds[k*(N//splits):(k+1)*(N//splits),:]
      py=np.mean(part,axis=0)
      scores=[]
      for i in range(part.shape[0]):
        pyx=part[i,:]
        scores.append(entropy(pyx,py))
      split_scores.append(np.exp(np.mean(scores)))
    mean=np.mean(split_scores)
    return mean

import numpy
from numpy import cov
from numpy import trace
from numpy import iscomplexobj
from numpy import asarray
from numpy.random import shuffle
from scipy.linalg import sqrtm
from keras.applications.inception_v3 import InceptionV3
from keras.applications.inception_v3 import preprocess_input
from keras.datasets.mnist import load_data
from skimage.transform import resize

def scale_images(images, new_shape):
 images_list = list()

 for image in images:
  new_image = resize(image, new_shape, 0)
  images_list.append(new_image)
 return asarray(images_list)

def calculate_fid(model, images1, images2):
 print("1")
 print(images1)
 print("1")
 print(images2)
 act1 = model.predict(images1)
 act2 = model.predict(images2)
 mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)
 mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)

 ssdiff = numpy.sum((mu1 - mu2)**2.0)

 covmean = sqrtm(sigma1.dot(sigma2))

 if iscomplexobj(covmean):
  covmean = covmean.real

 fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)
 return fid
def Fid(gmodel,images2):
  model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))

  loaderfi = DataLoader(train_set, batch_size, shuffle=True,
                           num_workers=4, persistent_workers=True, pin_memory=True)
  for i in loaderfi:
    images1 = torch.tensor(i[0])
    break

  images1 = (images1.clamp(-1, 1) + 1) / 2
  images1 = (images1 * 255).type(torch.uint8)




  images2 = images2.cpu()
  noise_images = noise_images.cpu()

  images1 = scale_images(images1, (299,299,3))
  images2 = scale_images(images2, (299,299,3))


  images1 = preprocess_input(images1)
  images2 = preprocess_input(images2)


  fid = calculate_fid(model, images1, images2)

  print('FID: %.3f' % fid)

  return fid

class EMA():
    def __init__(self, model, decay):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}

model = Net(n_steps=500, num_l=10, ch=32, ch_m=[1,2,2,2],
                     num_b=2, dropout=0.15).to(device)
criterion=nn.MSELoss
optimizer=optim.Adam(
            params=model.parameters(), lr=1e-4,
        )
ema = EMA(model, 0.999)
ema.register()

"""step = 100

def linear_beta_schedule(timesteps):
    scale = step / timesteps
    beta_start = scale * 0.0001
    beta_end = scale * 0.02
    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float64)
class GaussianDiffusion:
    def __init__(self, timesteps=step, beta_schedule='linear'):
      self.timesteps = timesteps

      if beta_schedule == 'linear':
          betas = linear_beta_schedule(timesteps)
      else:
          raise ValueError(f'unknown beta schedule {beta_schedule}')
      self.betas = betas

      self.alphas = 1. - self.betas
      self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)
      self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.)

      self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)
      self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)
      self.log_one_minus_alphas_cumprod = torch.log(1.0 - self.alphas_cumprod)
      self.sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod)
      self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / self.alphas_cumprod - 1)

      self.posterior_variance = (
          self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
      )

      self.posterior_log_variance_clipped = torch.log(
          torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]])
      )

      self.posterior_mean_coef1 = (
          self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
      )
      self.posterior_mean_coef2 = (
          (1.0 - self.alphas_cumprod_prev)
          * torch.sqrt(self.alphas)
          / (1.0 - self.alphas_cumprod)
      )

    def _extract(self, a, t, x_shape):
        batch_size = t.shape[0]
        out = a.to(t.device).gather(0, t).float()
        out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))
        return out
    @torch.no_grad()
    def ddim_sample(
        self,
        model,
        image_size=96,
        batch_size=8,
        channels=3,
        ddim_timesteps=50,
        ddim_discr_method="uniform",
        ddim_eta=0.0,
        clip_denoised=True):
        if ddim_discr_method == 'uniform':
            c = self.timesteps // ddim_timesteps
            ddim_timestep_seq = np.asarray(list(range(0, self.timesteps, c)))
        elif ddim_discr_method == 'quad':
            ddim_timestep_seq = (
                (np.linspace(0, np.sqrt(self.timesteps * .8), ddim_timesteps)) ** 2
            ).astype(int)
        else:
            raise NotImplementedError(f'There is no ddim discretization method called "{ddim_discr_method}"')
        ddim_timestep_seq = ddim_timestep_seq + 1
        ddim_timestep_prev_seq = np.append(np.array([0]), ddim_timestep_seq[:-1])

        device = next(model.parameters()).device
        label = torch.randint(0, 10, (batch_size,)).to(device) #add by myself
        sample_img = torch.randn((batch_size, channels, image_size, image_size), device=device)
        for i in tqdm(reversed(range(0, ddim_timesteps)), desc='sampling loop time step', total=ddim_timesteps):
            t = torch.full((batch_size,), ddim_timestep_seq[i], device=device, dtype=torch.long)
            prev_t = torch.full((batch_size,), ddim_timestep_prev_seq[i], device=device, dtype=torch.long)

            alpha_cumprod_t = self._extract(self.alphas_cumprod, t, sample_img.shape)
            alpha_cumprod_t_prev = self._extract(self.alphas_cumprod, prev_t, sample_img.shape)

            pred_noise = model(sample_img, t, label)

            pred_x0 = (sample_img - torch.sqrt((1. - alpha_cumprod_t)) * pred_noise) / torch.sqrt(alpha_cumprod_t)
            if clip_denoised:
                pred_x0 = torch.clamp(pred_x0, min=-1., max=1.)


            sigmas_t = ddim_eta * torch.sqrt(
                (1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_prev))

            pred_dir_xt = torch.sqrt(1 - alpha_cumprod_t_prev - sigmas_t**2) * pred_noise

            x_prev = torch.sqrt(alpha_cumprod_t_prev) * pred_x0 + pred_dir_xt + sigmas_t * torch.randn_like(sample_img)

            sample_img = x_prev

        return sample_img.cpu().numpy()



model=torch.load("./drive/MyDrive/model6cifarselfh")
diffusion = GaussianDiffusion()
sampled_images = diffusion.ddim_sample(model) #output 1 photo to calculate fid id

reverse_transforms = transforms.Compose([
    transforms.ToPILImage(),
])

plt.imshow(np.transpose((sampled_images[0].squeeze() + 0) * 255 , (1, 2, 0)))

class EMA:
    def __init__(self, beta):
        difies exponential moving average model.

        self.beta = beta
        self.step = 0

    def update_model_average(self, ema_model: nn.Module, current_model: nn.Module) -> None:
        for current_params, ema_params in zip(current_model.parameters(), ema_model.parameters()):
            old_weights, new_weights = ema_params.data, current_params.data
            ema_params.data = self.update_average(old_weights=old_weights, new_weights=new_weights)

    def update_average(self, old_weights: torch.Tensor, new_weights: torch.Tensor) -> torch.Tensor:
        if old_weights is None:
            return new_weights
        return old_weights * self.beta + (1 - self.beta) * new_weights

    def ema_step(self, ema_model: nn.Module, model: nn.Module, step_start_ema: int = 2000) -> None:
        if self.step < step_start_ema:
            self.reset_parameters(ema_model=ema_model, model=model)
            self.step += 1
            return
        self.update_model_average(ema_model=ema_model, current_model=model)
        self.step += 1

    @staticmethod
    def reset_parameters(ema_model: nn.Module, model: nn.Module) -> None:
        ema_model.load_state_dict(model.state_dict())
"""

from torch.optim.lr_scheduler import _LRScheduler

class GradualWarmupScheduler(_LRScheduler):
    def __init__(self, optimizer, multiplier, warm_epoch, after_scheduler=None):
        self.multiplier = multiplier
        self.total_epoch = warm_epoch
        self.after_scheduler = after_scheduler
        self.finished = False
        self.last_epoch = None
        self.base_lrs = None
        super().__init__(optimizer)

    def get_lr(self):
        if self.last_epoch > self.total_epoch:
            if self.after_scheduler:
                if not self.finished:
                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]
                    self.finished = True
                return self.after_scheduler.get_lr()
            return [base_lr * self.multiplier for base_lr in self.base_lrs]
        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]


    def step(self, epoch=None, metrics=None):
        if self.finished and self.after_scheduler:
            if epoch is None:
                self.after_scheduler.step(None)
            else:
                self.after_scheduler.step(epoch - self.total_epoch)
        else:
            return super(GradualWarmupScheduler, self).step(epoch)

"""from torchvision.utils import save_image
save_image(torch.from_numpy(sampled_images), os.path.join(
              "/content/drive/MyDrive/mymodelcifar10/",  "SampledGuidenceImgs1.png"), nrow=8)
sampled_images=sampled_images.transpose(0,2,3,1)*255
reverse_transforms = transforms.Compose([
  transforms.ToPILImage(),
])

if len(sampled_images.shape) == 4:
  image = sampled_images[3, :, :, :]
  plt.imshow(reverse_transforms(image.astype('uint8')))

model=torch.load("/content/drive/MyDrive/ddpmimagnetlabel5")
with torch.no_grad():
          step = int(22 // 11)
          labelList = []
          k = 0
          for i in range(1, 22 + 1):
              labelList.append(torch.ones(size=[1]).long() * k)
              if i % step == 0:
                  if k < 11 - 1:
                      k += 1
          labels = torch.cat(labelList, dim=0).long().to(device) + 1
          print("labels: ", labels)

          model.eval()
          sampler = GaussianDiffusionSampler(
              model, 0.0001, 0.02, 1000, w=1.8).to(device)
          # Sampled from standard normal distribution
          noisyImage = torch.randn(
              size=[22, 4, 64, 64], device=device)
          sampledImgs = sampler(noisyImage, labels)
          torch.save(sampledImgs, os.path.join(
            "/content/drive/MyDrive/", "sample_.pt"))

          save_image(torch.from_numpy(sampledImgs), os.path.join(
              "/content/drive/MyDrive/mymodelcifar10/",  "SampledGuidenceImgs1.png"), nrow=8)

model=torch.load("/content/drive/MyDrive/ddpmimagnetlabel5")
with torch.no_grad():
  for ep in  range(12):
          step = int(1000 // 10)
          labelList = []
          k = 0
          for i in range(1, 1000 + 1):
              labelList.append(torch.ones(size=[1]).long() * k)
              if i % step == 0:
                  if k < 10 - 1:
                      k += 1
          labels = torch.cat(labelList, dim=0).long().to(device) + 1
          print("labels: ", labels)

          model.eval()
          sampler = GaussianDiffusionSampler(
              model, 0.0001, 0.02, 1000, w=1.8).to(device)
          # Sampled from standard normal distribution
          noisyImage = torch.randn(
              size=[22, 4, 32, 32], device=device)
          sampledImgs = sampler(noisyImage, labels)
          output = sampledImgs * 0.5 + 0.5  # [0 ~ 1]

          if(ep == 0):
            out = output.to(device)
          else:
            out = torch.cat((out,output),0)
"""

from torchvision.utils import save_image
"""
model=torch.load("/content/drive/MyDrive/ddpmcifar10.pt")"""

def eval():
  with torch.no_grad():
          model.eval()
          step = int(80 // 10)
          labelList = []
          k = 0
          for i in range(1, 80 + 1):
              labelList.append(torch.ones(size=[1]) * k)
              if i % step == 0:
                  if k < 10 - 1:
                      k += 1
          labels = (torch.cat(labelList, dim=0).to(device) + 1).long()
          print("labels: ", labels)
          print(labels.type())
          model.eval()
          sampler = GaussianDiffusionSampler(
              model, 1e-4, 0.02, 500, w=1.8).to(device)
          # Sampled from standard normal distribution
          noisyImage = torch.randn(
              size=[80, 3, 32, 32], device=device)
          sampledImgs = sampler(noisyImage, labels)
          sampledImgs=torch.clamp(sampledImgs * 0.5 + 0.5, 0, 1)
          save_image(sampledImgs, os.path.join(
              "/content/drive/MyDrive/mymodelcifar10/",  "SampledGuidenceImgs1.png"), nrow=8)
          model.train()

"""image=sampledImgs.to(torch.float32)

image = (image / 2 + 0.5).clamp(0, 1)

image = image.detach().cpu().numpy()
save_image(torch.from_numpy(image), os.path.join(
              "/content/drive/MyDrive/mymodelcifar10/",  "SampledGuidenceImgs1.png"), nrow=8)
              """

from torchvision.utils import save_image
def save_images(images, path, **kwargs):
    grid = torchvision.utils.make_grid(images, **kwargs)
    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()
    im = Image.fromarray(ndarr)
    im.save(path)
mse = nn.MSELoss()
allmean=[]
Fids=[]

best_loss = float("inf")
for ep in range(6000):

  epoch_loss = 0.0
  model.train()
  turn=0
  iss=IS()
  cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(
  optimizer=optimizer, T_max=500, eta_min=0, last_epoch=-1)
  warmUpScheduler = GradualWarmupScheduler(optimizer=optimizer, multiplier=2.5,
                warm_epoch=500 // 10, after_scheduler=cosineScheduler)
  epoch_loss=0

  betas=torch.linspace(1e-4, 0.02, 500).float()
  alphas = 1. - betas
  alphas_bar = torch.cumprod(alphas, dim=0).to(device)
  sqrt_alphas_bar= torch.sqrt(alphas_bar).to(device)
  sqrt_one_minus_alphas_bar= torch.sqrt(1. - alphas_bar).to(device)
  to=0
  for images,labels in tqdm(loader):
    to+=1


    b = images.shape[0]
    optimizer.zero_grad()
    x_0 = images.to(device)
    labels = labels.to(device) + 1
    if np.random.rand() < 0.1:
        labels = torch.zeros_like(labels).to(device)
    t = torch.randint(500, size=(x_0.shape[0], )).to(device)
    noise = torch.randn_like(x_0)
    x_t =sqrt_alphas_bar[t].reshape(b,1,1,1).to(device)* x_0.to(device) +sqrt_one_minus_alphas_bar[t].reshape(b,1,1,1).to(device)* noise.to(device)
    loss = F.mse_loss(model(x_t, t,labels).to(torch.float32), noise.to(torch.float32), reduction='none')
    loss = loss.sum() / b**2.
    epoch_loss +=loss.item()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(
        model.parameters(), 1.)
    optimizer.step()

  warmUpScheduler.step()


  epoch_loss /=to
  torch.save(model,"/content/drive/MyDrive/ddpmcifar10.pt")
  print(ep," : ",epoch_loss)
  if ep%10==0:
    eval()





print(allmean)

fig, axes = plt.subplots(1, 1, figsize=(8, 4))
x=[]
for i in range(22):
  x.append(i)
# 折线图
axes.plot(x,allmean, linestyle='-', color='#DE6B58', marker='x', linewidth=1.5)

# 设置最小刻度间隔

# 展示图片
plt.show()

model=torch.load("/content/drive/MyDrive/model6cifar7")
sampled_images = diffusion.sample(model, 10)

reverse_transforms = transforms.Compose([
  transforms.ToPILImage(),
])

if len(sampled_images.shape) == 4:
  image = sampled_images[0, :, :, :]
  plt.imshow(reverse_transforms(image))

model=torch.load("/content/drive/MyDrive/model6cifar7")
diffusion = Diffusion(device=device)


fidd,noise_fid,sampled_images=Fid(model)

"""2.6552487110244565"""

model.eval()
model=torch.load("/content/drive/MyDrive/model2.h")
diffusion = Diffusion(device=device)
sampled_images = diffusion.sample(model, 1)
from torchvision import transforms
reverse_transforms = transforms.Compose([
        transforms.ToPILImage(),
    ])

if len(sampled_images.shape) == 4:
    image = sampled_images[0, :, :, :]
    plt.imshow(reverse_transforms(image))